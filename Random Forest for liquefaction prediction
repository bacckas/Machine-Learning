import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, roc_auc_score, roc_curve, auc, confusion_matrix
from sklearn.metrics import cohen_kappa_score
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.feature_selection import mutual_info_classif

# Define constants
TEST_SIZE = 0.3
RANDOM_STATE = 36

# Manually input hyperparameters

#N_ESTIMATORS = [100, 200, 300, 400, 500]
#MAX_DEPTH = [None, 10, 20, 30]
#min_samples_split: [2, 5, 10],
#min_samples_leaf: [1, 5, 10],
#max_features: ['auto', 'sqrt', 'log2']

n_estimators = 120
max_depth = 15
min_samples_split = 10
min_samples_leaf = 1
max_features = 'auto'

# Load the dataset from a CSV file
file_path = "C:\\Users\\LE\\Downloads\\THESIS\\Program\\CSV\\GRID_LABEL_KTMBASIN_classify_v2.2.csv"
data = pd.read_csv(file_path)

# Calculate correlation of the dataset
correlation_matrix = data.corr()
print("Correlation Matrix:")
print(correlation_matrix)

# Draw heatmap for correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='Blues')
plt.title("Correlation Matrix")
tick_marks = [i for i in range(len(data.columns))]
plt.xticks(tick_marks, data.columns, rotation=45)
plt.yticks(tick_marks, data.columns)
plt.tight_layout()
plt.show()

# Preprocess the data
#X = data[["NEAR_DIST", "sediment_thick", "vs30", "predom", "pga", "geomap"]]
X = data[["near_dis", "sediment_thick", "predom", "vs30", "pga", "geo_form"]]
y = data["LIQ"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)

# Calculate VIF
vif = pd.DataFrame()
vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
vif['features'] = X_train.columns
print("Variance Inflation Factor (VIF):")
print(vif)

# Perform hyperparameter tuning
param_grid = {
    'n_estimators': [n_estimators],
    'max_depth': [max_depth],
    'min_samples_split': [min_samples_split],
    'min_samples_leaf': [min_samples_leaf],
    'max_features': [max_features]
}
grid_search = GridSearchCV(RandomForestClassifier(random_state=RANDOM_STATE), param_grid, cv=5, scoring='f1_macro')
grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Select a model based on the problem's requirements
model_type = "classifier"
if model_type == "classifier":
    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=RANDOM_STATE, oob_score=True)
elif model_type == "regressor":
    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=RANDOM_STATE)

# Train the best model
best_model = grid_search.best_estimator_
best_model.fit(X_train, y_train)

# For Evaluation Metrics
y_pred_train = best_model.predict(X_train)
y_pred_test = best_model.predict(X_test)

# To Predict Probabilities For Liquefaction Susceptibility Mapping
y_pred_proba_train = best_model.predict_proba(X_train)
y_pred_proba_test = best_model.predict_proba(X_test)

# Print predicted probabilities
#print("Predicted Probabilities (Train):")
#print(y_pred_proba_train)
#print("Predicted Probabilities (Test):")
#print(y_pred_proba_test)

# Calculate feature importance using Information Gain Ratio
information_gain_ratio = mutual_info_classif(X_train, y_train)
print("Feature Importance (Information Gain Ratio):")
for i, importance in enumerate(information_gain_ratio):
    print(f"Feature {i+1}: {X.columns[i]} (importance: {importance:.3f})")

# Draw bar chart for feature importance using Information Gain Ratio
plt.figure(figsize=(10, 9))
plt.bar(range(len(information_gain_ratio)), information_gain_ratio, align="center")
plt.xticks(range(len(information_gain_ratio)), [{X.columns[i]} for i in range(len(information_gain_ratio))])
plt.xlabel("Input Parameters")
plt.ylabel("Importance")
plt.title("Feature Importance (Information Gain Ratio)")
for i,v in enumerate(information_gain_ratio):
    plt.text(i,v+0.01,f"{v:.3f}", ha="center", weight="bold")
plt.show()

# Evaluate the model
if model_type == "classifier":
    accuracy = accuracy_score(y_test, y_pred_test)
    precision = precision_score(y_test, y_pred_test)
    recall = recall_score(y_test, y_pred_test)
    f1 = f1_score(y_test, y_pred_test)
    auc_score_test = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])
    auc_error_test = 1 - auc_score_test
    kappa = cohen_kappa_score(y_test, y_pred_test)
    print(f"Accuracy: {accuracy:.3f}")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1 Score: {f1:.3f}")
    print(f"AUC Score (Test): {auc_score_test:.3f}")
    print(f"AUC Error (Test): {auc_error_test:.3f}")
    print(f"Cohen Kappa Score: {kappa:.3f}")
    
    # Plot AUROC curve for test data
    y_pred_proba_test = best_model.predict_proba(X_test)[:, 1]
    fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_proba_test)
    roc_auc_test = auc(fpr_test, tpr_test)
    
    # Plot AUROC curve for train data
    y_pred_proba_train = best_model.predict_proba(X_train)[:, 1]
    fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_proba_train)
    roc_auc_train = auc(fpr_train, tpr_train)
    
    # Plot AUROC curves for both test and train data
    plt.figure()
    plt.plot(fpr_test, tpr_test, color='darkorange', lw=2, label='ROC curve (Test, area = %0.2f)' % roc_auc_test)
    plt.plot(fpr_train, tpr_train, color='blue', lw=2, label='ROC curve (Train, area = %0.2f)' % roc_auc_train)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

elif model_type == "regressor":
    y_pred_test = best_model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred_test)
    rmse = np.sqrt(mse)
    print("Mean Squared Error:", mse)
    print("Root Mean Squared Error:", rmse)

    # Draw scatter plot of predicted vs actual values
    plt.figure(figsize=(8, 6))
    plt.scatter(y_test, y_pred_test)
    plt.xlabel("Actual Values")
    plt.ylabel("Predicted Values")
    plt.title("Predicted vs Actual Values")
    plt.show()

# Calculate confusion matrix
conf_mat = confusion_matrix(y_test, y_pred_test)
print("Confusion Matrix:")
print(conf_mat)

# Calculate true positive rate, false positive rate, true negative rate, false negative rate
tn, fp, fn, tp = conf_mat.ravel()
tpr = tp / (tp + fn)  # True positive rate
fpr = fp / (fp + tn)  # False positive rate
tnr = tn / (tn + fp)  # True negative rate
fnr = fn / (fn + tp)  # False negative rate

print(f"True Positive Rate: {tpr:.3f}")
print(f"False Positive Rate: {fpr:.3f}")
print(f"True Negative Rate: {tnr:.3f}")
print(f"False Negative Rate: {fnr:.3f}")

# Plot confusion matrix as a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt = '.0f')
plt.title("Confusion Matrix")
tick_marks = [i for i in range(2)]
plt.xticks(tick_marks, ["Non-liquefiable", "Liquefiable"], rotation=0)
plt.yticks(tick_marks, ["Non-liquefiable", "Liquefiable"])
plt.tight_layout()
plt.show()

### For exporting the prediction to the ARCGIS shapefile format

import geopandas as gpd
import numpy as np
import rasterio
import geocube
import rioxarray as rxr
from rasterio.transform import Affine
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from geocube.api.core import make_geocube

# Read the shapefile
gdf = gpd.read_file(r'D:\\ARC GIS\\KTM VALLEY\\WGS_1984_UTM_ZONE_45N\\GRID\\GRID_LABEL_KTMBASIN_Classify_v2.2.shp')

# Predict probabilities
y_pred_proba_test = best_model.predict_proba(X_test)

# Repeat the predictions to match the number of features
y_pred_proba_repeated = np.repeat(y_pred_proba_test[:, 1], len(gdf) // len(y_pred_proba_test) + 1)[:len(gdf)]

# Create a new DataFrame with the repeated predictions
df = pd.DataFrame(y_pred_proba_repeated, columns=['LSM'])

# Reset the index of the GeoDataFrame to create a unique index
gdf.reset_index(drop=True, inplace=True)

# Assign the repeated predictions to a new column in the GeoDataFrame
gdf['LSM'] = df['LSM']

# Set the CRS
gdf.set_crs(epsg=32645, inplace=True)

# Save the GeoDataFrame to a new shapefile
gdf.to_file(r'D:\\ARC GIS\\KTM VALLEY\\WGS_1984_UTM_ZONE_45N\\METHOD 2\\Classify_v2.2\\method2_grid_1_0.3_36_RFM_CLassify_v2.2.shp')

### FOR RASTER OUTPUT ###

# Get the overall bounds of the GeoDataFrame
minx, miny, maxx, maxy = gdf.total_bounds

# Convert the GeoPandas DataFrame to a raster
raster = make_geocube(vector_data=gdf, measurements=['LSM'], resolution=(10000, 10000))

# Save the raster to a file
raster.rio.to_raster("D:\\ARC GIS\\KTM VALLEY\\WGS_1984_UTM_ZONE_45N\METHOD 2\\Classify_v2.2\\method2_grid_1_0.3_36_RFM_CLassify_v2.2.tif")              
